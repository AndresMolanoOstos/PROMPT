{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb86412",
   "metadata": {},
   "source": [
    "# Modelo que determina la polaridad de una frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bddd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido a la aplicación de análisis de sentimientos.\n",
      "Por favor, ingresa una frase para analizar su sentimiento.\n",
      "Tu entrada: Today is hard\n",
      "1) negative 0.6613\n",
      "2) neutral 0.273\n",
      "3) positive 0.0657\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# Función para interactuar con el usuario de manera mejorada\n",
    "def interactuar_con_usuario():\n",
    "    # Instrucciones claras para el usuario\n",
    "    print(\"Bienvenido a la aplicación de análisis de sentimientos.\")\n",
    "    print(\"Por favor, ingresa una frase para analizar su sentimiento.\")\n",
    "\n",
    "    # Solicitar al usuario que ingrese una frase\n",
    "    while True:\n",
    "        try:\n",
    "            text = input(\"Tu entrada: \")\n",
    "            if not text:\n",
    "                raise ValueError(\"Por favor, ingresa una frase válida.\")\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "\n",
    "    text = preprocess(text)\n",
    "\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    # Print labels and scores\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = config.id2label[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "\n",
    "# Ejecutar la interacción con el usuario\n",
    "interactuar_con_usuario()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54800e",
   "metadata": {},
   "source": [
    "# Extracción de comentarios de Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a82ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(client_id=\"I80D4CWWdnBqVU_WEiZKjQ\", \n",
    "                     client_secret=\"pkLPDsbb8sn34LjFpP8gT-_GyQhqCA\",\n",
    "                     username=\"Andres_Molano_Ostos\", \n",
    "                     password=\"pamito197\", \n",
    "                     user_agent=\"Scrapper 1.0 by /u/Andres_Molano_Ostos\")\n",
    "\n",
    "# Subreddit y límite de comentarios\n",
    "subreddit = reddit.subreddit('Travel')\n",
    "limit = 1\n",
    "\n",
    "data = []\n",
    "for submission in subreddit.top(limit=limit):\n",
    "    submission.comments.replace_more(limit=1) # Para asegurarse de que se carguen todos los comentarios\n",
    "    for comment in submission.comments.list():\n",
    "        data.append({\n",
    "            'title': submission.title,\n",
    "            'score': submission.score,\n",
    "            'created_utc': submission.created_utc,\n",
    "            'num_comments': submission.num_comments,\n",
    "            'url': submission.url,\n",
    "            'comment_id': comment.id,\n",
    "            'comment_body': comment.body,\n",
    "            'comment_score': comment.score,\n",
    "            'comment_created_utc': comment.created_utc\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1aa19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpgnvj1</td>\n",
       "      <td>Thread locked due to the high number of rule-b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.614721e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpem2p4</td>\n",
       "      <td>Did you have a tour guide with you at all time...</td>\n",
       "      <td>826</td>\n",
       "      <td>1.614686e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpem0pi</td>\n",
       "      <td>Looks like perfectly normal photographs from t...</td>\n",
       "      <td>5684</td>\n",
       "      <td>1.614686e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpeo6jj</td>\n",
       "      <td>There's a Samsung AC condenser unit.</td>\n",
       "      <td>347</td>\n",
       "      <td>1.614687e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gperfs6</td>\n",
       "      <td>Only the leaders suffer from obesity..</td>\n",
       "      <td>383</td>\n",
       "      <td>1.614690e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpghgbh</td>\n",
       "      <td>His profile says he is active in r/australianp...</td>\n",
       "      <td>11</td>\n",
       "      <td>1.614718e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpgan8y</td>\n",
       "      <td>Honestly I did wonder about that.</td>\n",
       "      <td>7</td>\n",
       "      <td>1.614715e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpgl10v</td>\n",
       "      <td>yes they use it for plants as well.</td>\n",
       "      <td>8</td>\n",
       "      <td>1.614720e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpg06pw</td>\n",
       "      <td>I misread.</td>\n",
       "      <td>6</td>\n",
       "      <td>1.614711e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpflvkn</td>\n",
       "      <td>Oh my gosh, thank you so much for sharing!  Th...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.614705e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  score   created_utc  \\\n",
       "0    I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "1    I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "2    I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "3    I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "4    I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "..                                                 ...    ...           ...   \n",
       "581  I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "582  I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "583  I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "584  I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "585  I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "\n",
       "     num_comments                                    url comment_id  \\\n",
       "0            2768  https://www.reddit.com/gallery/lvzqh4    gpgnvj1   \n",
       "1            2768  https://www.reddit.com/gallery/lvzqh4    gpem2p4   \n",
       "2            2768  https://www.reddit.com/gallery/lvzqh4    gpem0pi   \n",
       "3            2768  https://www.reddit.com/gallery/lvzqh4    gpeo6jj   \n",
       "4            2768  https://www.reddit.com/gallery/lvzqh4    gperfs6   \n",
       "..            ...                                    ...        ...   \n",
       "581          2768  https://www.reddit.com/gallery/lvzqh4    gpghgbh   \n",
       "582          2768  https://www.reddit.com/gallery/lvzqh4    gpgan8y   \n",
       "583          2768  https://www.reddit.com/gallery/lvzqh4    gpgl10v   \n",
       "584          2768  https://www.reddit.com/gallery/lvzqh4    gpg06pw   \n",
       "585          2768  https://www.reddit.com/gallery/lvzqh4    gpflvkn   \n",
       "\n",
       "                                          comment_body  comment_score  \\\n",
       "0    Thread locked due to the high number of rule-b...              1   \n",
       "1    Did you have a tour guide with you at all time...            826   \n",
       "2    Looks like perfectly normal photographs from t...           5684   \n",
       "3                 There's a Samsung AC condenser unit.            347   \n",
       "4               Only the leaders suffer from obesity..            383   \n",
       "..                                                 ...            ...   \n",
       "581  His profile says he is active in r/australianp...             11   \n",
       "582                  Honestly I did wonder about that.              7   \n",
       "583                yes they use it for plants as well.              8   \n",
       "584                                         I misread.              6   \n",
       "585  Oh my gosh, thank you so much for sharing!  Th...              3   \n",
       "\n",
       "     comment_created_utc  \n",
       "0           1.614721e+09  \n",
       "1           1.614686e+09  \n",
       "2           1.614686e+09  \n",
       "3           1.614687e+09  \n",
       "4           1.614690e+09  \n",
       "..                   ...  \n",
       "581         1.614718e+09  \n",
       "582         1.614715e+09  \n",
       "583         1.614720e+09  \n",
       "584         1.614711e+09  \n",
       "585         1.614705e+09  \n",
       "\n",
       "[586 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dadb248",
   "metadata": {},
   "source": [
    "# Combinación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad60f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# ... (Código para cargar el modelo y funciones de procesamiento de texto)\n",
    "\n",
    "# Ajustar la longitud máxima de la secuencia de entrada\n",
    "max_sequence_length = 128  # Puedes ajustar esto según tus necesidades\n",
    "\n",
    "def obtener_porcentajes_sentimiento(texto):\n",
    "    texto = preprocess(texto)\n",
    "    encoded_input = tokenizer(texto, return_tensors='pt', max_length=max_sequence_length, truncation=True, padding=True)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame y crear las columnas correspondientes\n",
    "df['sentimiento_scores'] = df['comment_body'].apply(obtener_porcentajes_sentimiento)\n",
    "\n",
    "# Dividir los scores en columnas separadas\n",
    "df[['sentimiento_positivo', 'sentimiento_neutral', 'sentimiento_negativo']] = pd.DataFrame(df['sentimiento_scores'].tolist(), index=df.index)\n",
    "\n",
    "# Eliminar la columna 'sentimiento_scores' si ya no se necesita\n",
    "df = df.drop(columns=['sentimiento_scores'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec67b8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_created_utc</th>\n",
       "      <th>sentimiento_positivo</th>\n",
       "      <th>sentimiento_neutral</th>\n",
       "      <th>sentimiento_negativo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpgnvj1</td>\n",
       "      <td>Thread locked due to the high number of rule-b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.614721e+09</td>\n",
       "      <td>0.378245</td>\n",
       "      <td>0.599691</td>\n",
       "      <td>0.022064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpem2p4</td>\n",
       "      <td>Did you have a tour guide with you at all time...</td>\n",
       "      <td>826</td>\n",
       "      <td>1.614686e+09</td>\n",
       "      <td>0.013115</td>\n",
       "      <td>0.912336</td>\n",
       "      <td>0.074549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpem0pi</td>\n",
       "      <td>Looks like perfectly normal photographs from t...</td>\n",
       "      <td>5684</td>\n",
       "      <td>1.614686e+09</td>\n",
       "      <td>0.027856</td>\n",
       "      <td>0.431779</td>\n",
       "      <td>0.540365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpeo6jj</td>\n",
       "      <td>There's a Samsung AC condenser unit.</td>\n",
       "      <td>347</td>\n",
       "      <td>1.614687e+09</td>\n",
       "      <td>0.021405</td>\n",
       "      <td>0.763809</td>\n",
       "      <td>0.214785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gperfs6</td>\n",
       "      <td>Only the leaders suffer from obesity..</td>\n",
       "      <td>383</td>\n",
       "      <td>1.614690e+09</td>\n",
       "      <td>0.799163</td>\n",
       "      <td>0.188452</td>\n",
       "      <td>0.012386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpghgbh</td>\n",
       "      <td>His profile says he is active in r/australianp...</td>\n",
       "      <td>11</td>\n",
       "      <td>1.614718e+09</td>\n",
       "      <td>0.042842</td>\n",
       "      <td>0.885059</td>\n",
       "      <td>0.072099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpgan8y</td>\n",
       "      <td>Honestly I did wonder about that.</td>\n",
       "      <td>7</td>\n",
       "      <td>1.614715e+09</td>\n",
       "      <td>0.195447</td>\n",
       "      <td>0.728520</td>\n",
       "      <td>0.076032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpgl10v</td>\n",
       "      <td>yes they use it for plants as well.</td>\n",
       "      <td>8</td>\n",
       "      <td>1.614720e+09</td>\n",
       "      <td>0.008736</td>\n",
       "      <td>0.735500</td>\n",
       "      <td>0.255765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpg06pw</td>\n",
       "      <td>I misread.</td>\n",
       "      <td>6</td>\n",
       "      <td>1.614711e+09</td>\n",
       "      <td>0.388594</td>\n",
       "      <td>0.569510</td>\n",
       "      <td>0.041896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>I visited North Korea recently, these are some...</td>\n",
       "      <td>56804</td>\n",
       "      <td>1.614684e+09</td>\n",
       "      <td>2768</td>\n",
       "      <td>https://www.reddit.com/gallery/lvzqh4</td>\n",
       "      <td>gpflvkn</td>\n",
       "      <td>Oh my gosh, thank you so much for sharing!  Th...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.614705e+09</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>0.012678</td>\n",
       "      <td>0.983709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  score   created_utc  \\\n",
       "0    I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "1    I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "2    I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "3    I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "4    I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "..                                                 ...    ...           ...   \n",
       "581  I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "582  I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "583  I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "584  I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "585  I visited North Korea recently, these are some...  56804  1.614684e+09   \n",
       "\n",
       "     num_comments                                    url comment_id  \\\n",
       "0            2768  https://www.reddit.com/gallery/lvzqh4    gpgnvj1   \n",
       "1            2768  https://www.reddit.com/gallery/lvzqh4    gpem2p4   \n",
       "2            2768  https://www.reddit.com/gallery/lvzqh4    gpem0pi   \n",
       "3            2768  https://www.reddit.com/gallery/lvzqh4    gpeo6jj   \n",
       "4            2768  https://www.reddit.com/gallery/lvzqh4    gperfs6   \n",
       "..            ...                                    ...        ...   \n",
       "581          2768  https://www.reddit.com/gallery/lvzqh4    gpghgbh   \n",
       "582          2768  https://www.reddit.com/gallery/lvzqh4    gpgan8y   \n",
       "583          2768  https://www.reddit.com/gallery/lvzqh4    gpgl10v   \n",
       "584          2768  https://www.reddit.com/gallery/lvzqh4    gpg06pw   \n",
       "585          2768  https://www.reddit.com/gallery/lvzqh4    gpflvkn   \n",
       "\n",
       "                                          comment_body  comment_score  \\\n",
       "0    Thread locked due to the high number of rule-b...              1   \n",
       "1    Did you have a tour guide with you at all time...            826   \n",
       "2    Looks like perfectly normal photographs from t...           5684   \n",
       "3                 There's a Samsung AC condenser unit.            347   \n",
       "4               Only the leaders suffer from obesity..            383   \n",
       "..                                                 ...            ...   \n",
       "581  His profile says he is active in r/australianp...             11   \n",
       "582                  Honestly I did wonder about that.              7   \n",
       "583                yes they use it for plants as well.              8   \n",
       "584                                         I misread.              6   \n",
       "585  Oh my gosh, thank you so much for sharing!  Th...              3   \n",
       "\n",
       "     comment_created_utc  sentimiento_positivo  sentimiento_neutral  \\\n",
       "0           1.614721e+09              0.378245             0.599691   \n",
       "1           1.614686e+09              0.013115             0.912336   \n",
       "2           1.614686e+09              0.027856             0.431779   \n",
       "3           1.614687e+09              0.021405             0.763809   \n",
       "4           1.614690e+09              0.799163             0.188452   \n",
       "..                   ...                   ...                  ...   \n",
       "581         1.614718e+09              0.042842             0.885059   \n",
       "582         1.614715e+09              0.195447             0.728520   \n",
       "583         1.614720e+09              0.008736             0.735500   \n",
       "584         1.614711e+09              0.388594             0.569510   \n",
       "585         1.614705e+09              0.003612             0.012678   \n",
       "\n",
       "     sentimiento_negativo  \n",
       "0                0.022064  \n",
       "1                0.074549  \n",
       "2                0.540365  \n",
       "3                0.214785  \n",
       "4                0.012386  \n",
       "..                    ...  \n",
       "581              0.072099  \n",
       "582              0.076032  \n",
       "583              0.255765  \n",
       "584              0.041896  \n",
       "585              0.983709  \n",
       "\n",
       "[586 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c27f19",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0acbd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love is love.\n",
      "\n",
      "I'm not sure if I'm going to be able to say this, but I do know that I love you. I know you're a wonderful person. You're the best person I've ever known. And I want to thank you for everything you've done for me. It's been a long time coming. But I can't wait to see you again.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Cargar el modelo preentrenado y el tokenizador de GPT-2\n",
    "modelo = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizador = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Definir un prompt\n",
    "prompt = \"Love is love\"\n",
    "\n",
    "# Tokenizar el prompt\n",
    "input_ids = tokenizador.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generar una secuencia de texto completando el prompt\n",
    "output = modelo.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decodificar la secuencia generada\n",
    "texto_generado = tokenizador.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(texto_generado)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b589a71",
   "metadata": {},
   "source": [
    "# Combinación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3fe68f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué tipo de resumen de comentarios deseas?\n",
      "1. Resumen de comentarios positivos\n",
      "2. Resumen de comentarios negativos\n",
      "3. Resumen de comentarios neutros\n",
      "Ingresa el número de la opción: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen generado por GPT-2:\n",
      "A continuación se muestra un resumen de comentarios:\n",
      "Looks like perfectly normal photographs from the 70s.\n",
      "I'm not sure if this is a coincidence or not, but I'm sure it's a good thing. I think it is the first time I've seen a photograph of a man in a suit. It's not like I was expecting it to be a very good photograph. The man is wearing a white suit, and he's wearing the same clothes as the other men. He's in his suit and his hair is in the middle of the suit; he looks like he is going to have a lot of fun. And I thought it was a great photograph, because it shows the man's face.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "\n",
    "# Código para cargar el modelo de análisis de sentimiento\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# Código para cargar el modelo GPT-2 y el tokenizador\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ajustar la longitud máxima de la secuencia de entrada\n",
    "max_sequence_length = 128\n",
    "\n",
    "# Función para obtener los porcentajes de análisis de sentimiento\n",
    "def obtener_porcentajes_sentimiento(texto):\n",
    "    texto = preprocess(texto)\n",
    "    encoded_input = tokenizer(texto, return_tensors='pt', max_length=max_sequence_length, truncation=True, padding=True)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores\n",
    "\n",
    "# Función para generar un resumen de comentarios\n",
    "def generar_resumen_comentarios(df, tipo_sentimiento):\n",
    "    # Ordenar el DataFrame por comment_score en orden descendente y tomar los 10 comentarios con mayor puntaje\n",
    "    comentarios_seleccionados = df.sort_values(by='comment_score', ascending=False).head(1)\n",
    "    comentarios_seleccionados = comentarios_seleccionados['comment_body'].tolist()\n",
    "    \n",
    "    resumen = \"\"\n",
    "    \n",
    "    for comentario in comentarios_seleccionados:\n",
    "        resumen += comentario + \"\\n\"\n",
    "    \n",
    "    return resumen\n",
    "\n",
    "# ... (Código para cargar los comentarios en el DataFrame df)\n",
    "\n",
    "# Solicitar al usuario el tipo de resumen que desea\n",
    "print(\"¿Qué tipo de resumen de comentarios deseas?\")\n",
    "print(\"1. Resumen de comentarios positivos\")\n",
    "print(\"2. Resumen de comentarios negativos\")\n",
    "print(\"3. Resumen de comentarios neutros\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        opcion = int(input(\"Ingresa el número de la opción: \"))\n",
    "        if opcion not in [1, 2, 3]:\n",
    "            raise ValueError(\"Por favor, ingresa una opción válida.\")\n",
    "        break\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "if opcion == 1:\n",
    "    tipo_sentimiento = 'sentimiento_positivo'\n",
    "elif opcion == 2:\n",
    "    tipo_sentimiento = 'sentimiento_negativo'\n",
    "else:\n",
    "    tipo_sentimiento = 'sentimiento_neutral'\n",
    "\n",
    "# Generar el resumen de los 10 comentarios con mayor puntaje\n",
    "resumen = generar_resumen_comentarios(df, tipo_sentimiento)\n",
    "\n",
    "# Generar un resumen con GPT-2\n",
    "prompt = \"A continuación se muestra un resumen de comentarios:\"\n",
    "input_ids = gpt2_tokenizer.encode(prompt + \"\\n\" + resumen, return_tensors=\"pt\")\n",
    "output = gpt2_model.generate(input_ids, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "resumen_gpt2 = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Imprimir el resumen generado por GPT-2\n",
    "print(\"\\nResumen generado por GPT-2:\")\n",
    "print(resumen_gpt2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79060790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
